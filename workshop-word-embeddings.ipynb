{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from glob import glob\n",
    "# import word2vec\n",
    "# import gensim\n",
    "# from gensim.test.utils import common_texts\n",
    "# from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "from mpl_toolkits.mplot3d import Axes3D, proj3d\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "%matplotlib notebook\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Topics in Word Embeddings\n",
    "\n",
    "## NYCDH Week, February 2019\n",
    "\n",
    "Here are some more resources for getting started: \n",
    "\n",
    "- [A classic primer on Word Embeddings, from Google (uses TensorFlow)](https://www.tensorflow.org/tutorials/representation/word2vec)\n",
    "- [Another word2vec tutorial using TensorFlow](https://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/)\n",
    "- [The original documentation of word2vec](https://code.google.com/archive/p/word2vec/)\n",
    "- [Spacy Docs on vector similarity](https://spacy.io/usage/vectors-similarity)\n",
    "- [Gensim Docs](https://radimrehurek.com/gensim/models/keyedvectors.html)\n",
    "\n",
    "\n",
    "For getting started in NLP more generally, here are two notebooks of mine: \n",
    "\n",
    "- [Introduction to Text Analysis](https://github.com/JonathanReeve/dataviz-workshop-2017)\n",
    "- [Advanced Text Analysis Workshop](https://github.com/JonathanReeve/advanced-text-analysis-workshop-2017)\n",
    "\n",
    "## An Example of Document Vectors: Project Gutenberg\n",
    "\n",
    "![First 30 Books of Project Gutenberg](example-gut.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vector Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "africanSwallow = nlp('African swallow')\n",
    "europeanSwallow = nlp('European swallow')\n",
    "coconut = nlp('coconut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "africanSwallow.similarity(europeanSwallow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "africanSwallow.similarity(coconut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(vecA, vecB):\n",
    "    return dot(vecA, vecB) / (norm(vecA, ord=2) * norm(vecB, ord=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity(africanSwallow.vector, europeanSwallow.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swallowArithmetic = (africanSwallow.vector - europeanSwallow.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostSimilar(vec):\n",
    "    highestSimilarities = [0]\n",
    "    highestWords = [\"\"]\n",
    "    for w in nlp.vocab:\n",
    "        sim = similarity(vec, w.vector)\n",
    "        if sim > highestSimilarities[-1]:\n",
    "            highestSimilarities.append(sim)\n",
    "            highestWords.append(w.text.lower())\n",
    "    return list(zip(highestWords, highestSimilarities))[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analogies (Linear Algebra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostSimilar(swallowArithmetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostSimilar(coconut.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "king, queen, woman, man = [nlp(w).vector for w in ['king', 'queen', 'woman', 'man']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = king - man + woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostSimilar(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paris, france, germany = [nlp(w).vector for w in ['Paris', 'France', 'Germany']]\n",
    "answer = paris - france + germany\n",
    "mostSimilar(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pride and Prejudice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pride = open('pride.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prideDoc = nlp(pride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prideNouns = [w for w in prideDoc if w.pos_.startswith('N')][:40]\n",
    "prideNounLabels = [w.lemma_ for w in prideNouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prideNounVecs = [w.vector for w in prideNouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced = PCA(n_components=3).fit_transform(prideNounVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prideDF = pd.DataFrame(reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
    "\n",
    "def plotResults3D(df, labels): \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(df[0], df[1], df[2], marker='o')\n",
    "    for i, label in enumerate(labels):\n",
    "        ax.text(df.loc[i][0], df.loc[i][1], df.loc[i][2], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotResults3D(prideDF, prideNounLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo this function with only nouns from Pride and Prejudice\n",
    "def mostSimilar(vec):\n",
    "    highestSimilarities = [0]\n",
    "    highestWords = [\"\"]\n",
    "    for w in prideNouns:\n",
    "        sim = similarity(vec, w.vector)\n",
    "        if sim > highestSimilarities[-1]:\n",
    "            highestSimilarities.append(sim)\n",
    "            highestWords.append(w.text.lower())\n",
    "    return list(zip(highestWords, highestSimilarities))[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostSimilar(nlp('wife').vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senseDocs = [nlp(w) for w in ['sound', 'sight', 'touch', 'smell']]\n",
    "def whichSense(word):\n",
    "    doc = nlp(word)\n",
    "    return {sense: doc.similarity(sense) for sense in senseDocs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whichSense('symphony')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (14,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testWords = 'symphony itchy flower crash'.split()\n",
    "pd.DataFrame([whichSense(w) for w in testWords], index=testWords).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Inaugural Address Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inauguralFilenames = sorted(glob('inaugural/*'))\n",
    "inauguralLabels = [fn[10:-4] for fn in inauguralFilenames]\n",
    "inauguralDates = [int(label[:4]) for label in inauguralLabels]\n",
    "parties = 'rrrbbrrrbbbbbrrbbrrbrrrbbrrbr' # I did this manually. There are probably errors.\n",
    "inauguralRaw = [open(f, errors=\"ignore\").read() for f in inauguralFilenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: peek\n",
    "for i in range(4): \n",
    "    print(inauguralLabels[i][:30], inauguralDates[i], inauguralRaw[i][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inauguralDocs = [nlp(text) for text in inauguralRaw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inauguralVecs = [doc.vector for doc in inauguralDocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a similarity matrix.\n",
    "# Check the similarity of everything against everything else. \n",
    "# Note: this is not very efficient. \n",
    "similarities = []\n",
    "for vec in inauguralDocs: \n",
    "    thisSimilarities = [vec.similarity(other) for other in inauguralDocs]\n",
    "    similarities.append(thisSimilarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(similarities, columns=inauguralLabels, index=inauguralLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df < 1].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = PCA(n_components=2).fit_transform(inauguralVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = embedded[:,0], embedded[:,1]\n",
    "for i in range(len(xs)): \n",
    "    plt.scatter(xs[i], ys[i], c=parties[i], s=inauguralDates[i]-1900)\n",
    "    plt.annotate(inauguralLabels[i], (xs[i], ys[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detective Novels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectiveJSON = open('detectives.json')\n",
    "detectivesData = json.load(detectiveJSON)\n",
    "detectivesData = detectivesData[1:] # Chop of #1, which is actually a duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectiveTexts = [book['text'] for book in detectivesData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectiveLengths = [len(text) for text in detectiveTexts] # How long are they? We might want to cut them down\n",
    "detectiveLengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectiveTextsTruncated = [t[:min(detectiveLengths)] for t in detectiveTexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectiveDocs = [nlp(book) for book in detectiveTextsTruncated] # This should take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraWords = \"gun knife snake diamond\".split()\n",
    "extraDocs = [nlp(word) for word in extraWords]\n",
    "extraVecs = [doc.vector for doc in extraDocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectiveVecs = [doc.vector for doc in detectiveDocs]\n",
    "detectiveLabels = [doc['author'].split(',')[0] +  '-' + doc['title'][:20] for doc in detectivesData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectiveLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaOut = PCA(n_components=10).fit_transform(detectiveVecs + extraVecs)\n",
    "tsneOut = TSNE(n_components=2).fit_transform(pcaOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xs, ys = tsneOut[:,0], tsneOut[:,1]\n",
    "for i in range(len(xs)): \n",
    "    plt.scatter(xs[i], ys[i])\n",
    "    plt.annotate((detectiveLabels + extraWords)[i], (xs[i], ys[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "counts = {label: {w: 0 for w in extraWords} for label in detectiveLabels}\n",
    "for i, doc in enumerate(detectiveDocs):\n",
    "    for w in doc: \n",
    "        if w.lemma_ in extraWords: \n",
    "            counts[detectiveLabels[i]][w.lemma_] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(counts).T.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Your Own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires Gensim. See commented-out imports above.\n",
    "model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in dir(word_vectors) if not w.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
